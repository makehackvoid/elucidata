{
 "metadata": {
  "name": "data_import"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Import the data to postgresql"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "import json\n",
      "import requests\n",
      "from sqlalchemy import *\n",
      "from sqlalchemy.orm import create_session\n",
      "from sqlalchemy.ext.declarative import declarative_base\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "import dateutil.parser\n",
      "import time\n",
      "import regex\n",
      "from collections import Counter\n",
      "import types\n",
      "\n",
      "stop_words = '''about,after,all,also,an,and,another,any,are,as,at,be,\n",
      "because,been,before,being,between,both,but,by,came,can,come,could,did,\n",
      "do,each,for,from,get,got,has,had,he,have,her,here,him,himself,his,how,\n",
      "if,in,into,is,it,like,make,many,me,might,more,most,much,must,my,never,\n",
      "now,of,on,only,or,other,our,out,over,said,same,see,should,since,some,\n",
      "still,such,take,than,that,the,their,them,then,there,these,they,this,\n",
      "those,through,to,too,under,up,very,was,way,we,well,were,what,where,\n",
      "which,while,who,with,would,you,your,\n",
      "nan,inc,http,www,gov,com,mailto,aspx,yes,no,excl,etc,org,edu'''\n",
      "stop_words = stop_words.replace('\\n', ',')\n",
      "stop_words = stop_words.split(',')\n",
      "\n",
      "api_url = 'http://opendata.linkdigital.com.au/api/3/'\n",
      "\n",
      "get_resources = 'http://opendata.linkdigital.com.au/api/3/action/current_package_list_with_resources'\n",
      "\n",
      "live = True\n",
      "\n",
      "if live:\n",
      "    print 'Running in live mode'\n",
      "    r = requests.get(get_resources)\n",
      "    with open('data.json', 'w') as f:\n",
      "        f.write(r.content)\n",
      "        f.close()\n",
      "    raw_data = r.content\n",
      "else:\n",
      "    print 'Running in development mode, data.json file must exist'\n",
      "    with open('data.json', 'r') as f:\n",
      "        raw_data = f.read()\n",
      "        f.close()\n",
      "    raw_data = raw_data.decode('iso-8859-1').encode('ascii', 'ignore')\n",
      "\n",
      "data = json.loads(raw_data)\n",
      "\n",
      "data_sets = pandas.DataFrame(data['result'])\n",
      "\n",
      "# strip fields we don't need\n",
      "data_sets = data_sets[[\n",
      "    # 'author',\n",
      "    # 'author_email',\n",
      "    # 'extras',\n",
      "    'groups',\n",
      "    'id',\n",
      "    # 'isopen',\n",
      "    # 'license_id',\n",
      "    # 'license_title',\n",
      "    # 'license_url',\n",
      "    # 'maintainer',\n",
      "    # 'maintainer_email',\n",
      "    # 'metadata_created',\n",
      "    # 'metadata_modified',\n",
      "    'name',\n",
      "    'notes',\n",
      "    'num_resources',\n",
      "    'num_tags',\n",
      "    'organization',\n",
      "    # 'owner_org',\n",
      "    # 'private',\n",
      "    # 'relationships_as_object',\n",
      "    # 'relationships_as_subject',\n",
      "    'resources',\n",
      "    'revision_id',\n",
      "    'revision_timestamp',\n",
      "    # 'state',\n",
      "    'tags',\n",
      "    'title',\n",
      "    # 'tracking_summary',\n",
      "    'type',\n",
      "    # 'url',\n",
      "    # 'version'\n",
      "    ]]\n",
      "\n",
      "# TODO: move database details into config file not held in repository\n",
      "user = 'postgres'\n",
      "password = ''\n",
      "server = 'localhost'\n",
      "port = '5432'\n",
      "database = 'mhv-govhack'\n",
      "databaseurl = 'postgresql://' + user + ':' + password + '@' + server + ':' + port + '/' + database\n",
      "\n",
      "# tell engine that database is using utf8\n",
      "engine = create_engine(databaseurl, client_encoding='utf8')\n",
      "session = create_session(bind=engine)\n",
      "Base = declarative_base()\n",
      "metadata = MetaData(bind=engine)\n",
      "\n",
      "\n",
      "class DataSet(Base):\n",
      "    __table__ = Table('dataset', metadata, autoload=True)\n",
      "\n",
      "\n",
      "class Coordinate(Base):\n",
      "    __table__ = Table('coordinate', metadata, autoload=True)\n",
      "\n",
      "\n",
      "class DataPoint(Base):\n",
      "    __table__ = Table('datapoint', metadata, autoload=True)\n",
      "\n",
      "\n",
      "class ResourceMap(Base):\n",
      "    __table__ = Table('resource_map', metadata, autoload=True)\n",
      "\n",
      "\n",
      "class ResourceText(Base):\n",
      "    __table__ = Table('resource_text', metadata, autoload=True)\n",
      "\n",
      "\n",
      "class Word(Base):\n",
      "    __table__ = Table('word', metadata, autoload=True)\n",
      "\n",
      "\n",
      "class Tag(Base):\n",
      "    __table__ = Table('tag', metadata, autoload=True)\n",
      "\n",
      "\n",
      "class Group(Base):\n",
      "    __table__ = Table('group', metadata, autoload=True)\n",
      "\n",
      "\n",
      "def usable_format(resources):\n",
      "    # All formats in data_sets = set([u'XML', u'xlsx', u'ZIP', u'plain', u'KML', u'PDF', u'CSV', u'XLS', u'shp'])\n",
      "    # TODO: add formats other than csv\n",
      "    # if there are no valid formats in the data_set, we don't want to create the dataset record\n",
      "    for resource in resources:\n",
      "        if resource['format'].lower() in ['csv', 'kml']:\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "def text_format(resource):\n",
      "    if resource['format'].lower() in ['xml', 'csv']:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "for key, data_set in data_sets.iterrows():\n",
      "    if usable_format(data_set['resources']):\n",
      "        session.begin()\n",
      "\n",
      "        # fill the dataset table\n",
      "        # TODO: check that each field exists in data_set\n",
      "        dataset = DataSet()\n",
      "        dataset.id = data_set['id']\n",
      "        dataset.api_url = api_url\n",
      "        dataset.name = data_set['title']\n",
      "        dataset.url_name = data_set['name']\n",
      "        if 'organisation' in data_set.keys():\n",
      "            dataset.provider = data_set['organization']['title']\n",
      "        dataset.revision_date = time.mktime(dateutil.parser.parse(\n",
      "                data_set['revision_timestamp']).timetuple()\n",
      "            )\n",
      "        dataset.revision_id = data_set['revision_id']\n",
      "        note = data_set['notes']\n",
      "        if note.startswith('    \\n\\n'):\n",
      "            note = note.replace('    \\n\\n','')\n",
      "            note = note[0:note.find('\\n\\n')]\n",
      "        dataset.description = note\n",
      "        session.add(dataset)\n",
      "\n",
      "        # have to commit the main record before the child tables. Would 'relationships' help with this?\n",
      "        try:\n",
      "            session.commit()\n",
      "        except SQLAlchemyError: # already exists (hopefully)\n",
      "            # TODO: compare existing DataSet record's revision date && id to see if we need to replace it\n",
      "            # code not working\n",
      "            # q = session.query(DataSet).filter(DataSet.revision_id == dataset.revision_id)\n",
      "            # if session.query(q.exists()):\n",
      "            #    print 'it exists!', data_set['revision_id']\n",
      "            #    continue\n",
      "            # else:\n",
      "            #    session.delete(dataset)\n",
      "            #    session.add(dataset)\n",
      "            #    session.commit()\n",
      "            session.close()\n",
      "            continue\n",
      "\n",
      "        session.begin()\n",
      "        # fill the group table\n",
      "        for a_group in data_set['groups']:\n",
      "            group = Group()\n",
      "            # TODO: not sure if we can set the dataset_id outside the loop?\n",
      "            group.dataset_id = data_set['id']\n",
      "            group.group = a_group['title']\n",
      "            session.add(group)\n",
      "\n",
      "        # fill the tag table\n",
      "        for a_tag in data_set['tags']:\n",
      "            tag = Tag()\n",
      "            # TODO: not sure if we can set the dataset_id outside the loop?\n",
      "            tag.dataset_id = data_set['id']\n",
      "            tag.tag = a_tag['name']\n",
      "            session.add(tag)\n",
      "\n",
      "        session.commit()\n",
      "        session.close()\n",
      "\n",
      "        # fill the resource_text or resource_map table\n",
      "        for resource in data_set['resources']:\n",
      "            if text_format(resource):\n",
      "                resource_text = ResourceText()\n",
      "                resource_text.id = resource['id']\n",
      "                resource_text.dataset_id = data_set['id']\n",
      "                resource_text.name = resource['name']\n",
      "                resource_text.resource_url = resource['url']\n",
      "                resource_text.type = resource['format']\n",
      "                if resource['format'].lower() == 'csv':\n",
      "                    cnt = Counter()\n",
      "                    try:\n",
      "                        # TODO: check that the data.gov data\n",
      "                        # is using utf8 encoding\n",
      "                        df = pandas.DataFrame.from_csv(resource['url'], encoding='utf8')\n",
      "                    except:\n",
      "                        resource_text.parsed = False\n",
      "                        session.begin()\n",
      "                        session.add(resource_text)\n",
      "                        session.commit()\n",
      "                        continue\n",
      "                    # at the moment this is stripping any column or row\n",
      "                    # headers that are just numbers\n",
      "                    resource_text.column_headers = ', '.join([s for s in df.columns.tolist() if isinstance(s, types.StringTypes)])\n",
      "                    resource_text.row_headers = ', '.join([s for s in df.index.tolist() if isinstance(s, types.StringTypes)])\n",
      "                    a_words = regex.sub('[^a-zA-Z ]+',' ', df.to_string())\n",
      "                    for a_word in a_words.split():\n",
      "                        a_word = a_word.lower()\n",
      "                        # TODO: add stemming / abbreviation expansion\n",
      "                        # (eg. dept -> department)\n",
      "                        if a_word not in stop_words:\n",
      "                            if len(a_word) > 2:\n",
      "                                cnt[a_word] += 1\n",
      "                    resource_text.wordcount = sum(cnt.values())\n",
      "                    resource_text.parsed = True\n",
      "\n",
      "                    session.begin()\n",
      "                    session.add(resource_text)\n",
      "                    session.commit()\n",
      "                    #if len(cnt.items()) > 0:\n",
      "                    session.begin()\n",
      "                    # get the top 30 words and put htem into word\n",
      "                    for w in cnt.most_common(30):\n",
      "                        word = Word()\n",
      "                        word.word = w[0]\n",
      "                        word.frequency = w[1]*1.0/resource_text.wordcount\n",
      "                        word.text_id = resource['id']\n",
      "                        # add stemming of words\n",
      "                        session.add(word)\n",
      "                    session.commit()\n",
      "\n",
      "                # elif resource['format'].lower() == 'xml':\n",
      "                    # TODO Add XML support\n",
      "                    # None\n",
      "                # else:\n",
      "                    # print 'unexpected text format:', resource['format']\n",
      "\n",
      "            else:\n",
      "                resource_map = ResourceMap()\n",
      "                resource_map.id = resource['id']\n",
      "                resource_map.dataset_id = data_set['id']\n",
      "                resource_map.name = resource['name']\n",
      "                resource_map.resource_url = resource['url']\n",
      "                resource_map.type = resource['format']\n",
      "                # TODO: have to get the actual resources using it's url\n",
      "                # and then parse it based on type\n",
      "                if resource['format'].lower() == 'kml':\n",
      "                    # TODO: parse kml data and\n",
      "                    # fill datapoint and coordinate tables\n",
      "                    # print resource['url']\n",
      "                    None\n",
      "                # else:\n",
      "                    # print 'unexpected text format:', resource['format']\n",
      "                session.begin()\n",
      "                session.add(resource_map)\n",
      "                session.commit()\n",
      "\n",
      "session.close()\n",
      "\n",
      "print \"data import finished\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "check the unique types of resources in data_sets ( resources 'format' field)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = set()\n",
      "for resource in data_sets['resources']:\n",
      "    for item in resource:\n",
      "        if item['format'] not in output:\n",
      "            output.add(item['format'])\n",
      "print output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'XML', u'xlsx', u'ZIP', u'plain', u'KML', u'PDF', u'CSV', u'XLS', u'shp'])\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the usable_format function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def usable_format(resources):\n",
      "    # All formats in data_sets = set([u'XML', u'xlsx', u'ZIP', u'plain', u'KML', u'PDF', u'CSV', u'XLS', u'shp'])\n",
      "    # just start with these, add more if we have time\n",
      "    # TODO: if there are no valid formats in the data_set, we probably don't want to create the dataset record at all?\n",
      "    for resource in resources:\n",
      "        if resource['format'].lower() in ['xml','csv','kml','csv', 'shp']:\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "i = 0\n",
      "for key, data_set in data_sets.iterrows():\n",
      "    if usable_format(data_set['resources']):\n",
      "        i += 1\n",
      "    else:\n",
      "        None\n",
      "        # print key, 'no usable resource'\n",
      "        \n",
      "print 'number of usable resources', i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of usable resources 623\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "groups                [{u'capacity': u'public', u'description': u'A ...\n",
        "id                                 3b715176-03f4-49c3-a792-76723f083573\n",
        "name                          data-gov-au-beta-hits-during-govhack-2013\n",
        "notes                                                                  \n",
        "num_resources                                                         2\n",
        "num_tags                                                              0\n",
        "organization          {u'description': u'Department of Finance and D...\n",
        "resources             [{u'resource_group_id': u'f295e4c5-ea54-4d8a-8...\n",
        "revision_id                        c1f5ba98-1b4e-4476-8baa-36f605de9e07\n",
        "revision_timestamp                           2013-06-02T01:28:02.307335\n",
        "tags                                                                 []\n",
        "title                         Data.gov.au Beta Hits during GovHack 2013\n",
        "type                                                            dataset\n",
        "Name: 0, dtype: object"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}