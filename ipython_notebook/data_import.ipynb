{
 "metadata": {
  "name": "data_import"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Import the data to postgresql"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "import json\n",
      "import requests\n",
      "from sqlalchemy import *\n",
      "from sqlalchemy.orm import create_session\n",
      "from sqlalchemy.ext.declarative import declarative_base\n",
      "\n",
      "api_url = 'http://opendata.linkdigital.com.au/api/3/'\n",
      "    \n",
      "get_resources = 'http://opendata.linkdigital.com.au/api/3/action/current_package_list_with_resources'\n",
      "\n",
      "live = False\n",
      "\n",
      "if live:\n",
      "    r = requests.get(get_resources)\n",
      "    with open('data.json', 'w') as f:\n",
      "        f.write(r.content)\n",
      "        f.close()\n",
      "    raw_data = r.content\n",
      "else:\n",
      "    with open('data.json', 'r') as f:\n",
      "        raw_data = f.read()\n",
      "        f.close()\n",
      "    \n",
      "    data = json.loads(raw_data)\n",
      "\n",
      "# print data.keys()\n",
      "\n",
      "data_sets = pandas.DataFrame(data['result'])\n",
      "\n",
      "#for name in df.columns:\n",
      "#    print '\\t\\'' + name + '\\','\n",
      "    \n",
      "data_sets = data_sets[[\n",
      "    # 'author',\n",
      "\t# 'author_email',\n",
      "\t# 'extras',\n",
      "\t'groups',\n",
      "\t'id',\n",
      "\t# 'isopen',\n",
      "\t# 'license_id',\n",
      "\t# 'license_title',\n",
      "\t# 'license_url',\n",
      "\t# 'maintainer',\n",
      "\t# 'maintainer_email',\n",
      "\t# 'metadata_created',\n",
      "\t# 'metadata_modified',\n",
      "\t'name',\n",
      "\t# 'notes',\n",
      "\t'num_resources',\n",
      "\t'num_tags',\n",
      "\t'organization',\n",
      "\t# 'owner_org',\n",
      "\t# 'private',\n",
      "\t# 'relationships_as_object',\n",
      "\t# 'relationships_as_subject',\n",
      "\t'resources',\n",
      "\t'revision_id',\n",
      "\t'revision_timestamp',\n",
      "\t# 'state',\n",
      "\t'tags',\n",
      "\t'title',\n",
      "\t# 'tracking_summary',\n",
      "\t'type',\n",
      "\t# 'url',\n",
      "\t# 'version'\n",
      "    ]]\n",
      "\n",
      "#SQLAlchemy\n",
      "# change when making .py file\n",
      "\n",
      "user = 'postgres'\n",
      "password = ''\n",
      "server = 'localhost'\n",
      "port = '5432'\n",
      "database = 'mhv-govhack'\n",
      "engine = create_engine('postgresql://' + user + ':' + password + '@' + server + ':' + port + '/' + database)\n",
      "\n",
      "Base = declarative_base()\n",
      "metadata = MetaData(bind=engine)\n",
      "\n",
      "class DataSet(Base):\n",
      "    __table__ = Table('dataset', metadata, autoload=True)\n",
      "\n",
      "class Coordinate(Base):\n",
      "    __table__ = Table('coordinate', metadata, autoload=True)\n",
      "\n",
      "class DataPoint(Base):\n",
      "    __table__ = Table('datapoint', metadata, autoload=True)\n",
      "    \n",
      "class ResourceMap(Base):\n",
      "    __table__ = Table('resource_map', metadata, autoload=True)\n",
      "    \n",
      "class ResourceText(Base):\n",
      "    __table__ = Table('resource_text', metadata, autoload=True)\n",
      "    \n",
      "class Word(Base):\n",
      "    __table__ = Table('word', metadata, autoload=True)\n",
      "    \n",
      "class Tag(Base):\n",
      "    __table__ = Table('tag', metadata, autoload=True)\n",
      "    \n",
      "class Group(Base):\n",
      "    __table__ = Table('group', metadata, autoload=True)\n",
      "\n",
      "for key, data_set in data_sets.iterrows():\n",
      "    # fill the dataset table\n",
      "    dataset = DataSet()  \n",
      "    dataset.id = data_set['id']\n",
      "    dataset.api_url = api_url\n",
      "    dataset.name = data_set['name']\n",
      "    dataset.provider = data_set['organization']['title']\n",
      "    dataset.revision_date = data_set['revision_timestamp']\n",
      "    dataset.revision_id = data_set['revision_id']\n",
      "    # session.add(dataset)\n",
      "    \n",
      "    # TODO: fill the group table\n",
      "    # TODO: fill the tag table\n",
      "    \n",
      "    # fill the resource_text table\n",
      "    resource_text = ResourceText()\n",
      "    usable_format = False\n",
      "    for resource in data_set['resources']:\n",
      "        # All formats in data_sets = set([u'XML', u'xlsx', u'ZIP', u'plain', u'KML', u'PDF', u'CSV', u'XLS', u'shp'])\n",
      "        # just start with these, add more if we have time\n",
      "        # TODO: if there are no valid formats in the data_set, we probably don't want to create the dataset record at all?\n",
      "        if resource['format'].lower() in ['xml','csv','kml','csv', 'shp']:\n",
      "            resource_text.id = resource['id']\n",
      "            resource_text.name = resource['name']\n",
      "            resource_text.resource_url = resource['url']\n",
      "            # session.add(resource_text)\n",
      "            usable_format = True\n",
      "            \n",
      "            # TODO: have to get the actual resources here from it's url and then parse it based on type\n",
      "            \n",
      "            # TODO: store the parsed data in the other data tables \n",
      "    break\n",
      "\n",
      "# TODO: need to decide where/ how often to do the commit.\n",
      "# session.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "check the unique types of resources in data_sets ( resources 'format' field)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = set()\n",
      "for resource in data_sets['resources']:\n",
      "    for item in resource:\n",
      "        if item['format'] not in output:\n",
      "            output.add(item['format'])\n",
      "print output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'XML', u'xlsx', u'ZIP', u'plain', u'KML', u'PDF', u'CSV', u'XLS', u'shp'])\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}